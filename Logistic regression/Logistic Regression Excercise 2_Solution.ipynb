{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwGWNHs2xIsx"
   },
   "source": [
    "# Logistic Regression Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcaBeePgu_Tu"
   },
   "source": [
    "## Multi-class classification of MNIST using Logistic Regression\n",
    "\n",
    "The multi-class scenario for logistic regression is quite similar to the binary case, except that the label $y$ is now an integer in {1, ...., K} where $K$ is the number of classes. In this excercise you will be provided with handwritten digit images. Write the code and compute the test accuracy by training a logistic regression based classifier in (i) one-vs-one, and (ii) one-vs-all setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        #print('searching: %s'%nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        #print('searching: %s' % nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        #print('Found %d cells'%len(nb.cells))\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "#  register the NotebookFinder with sys.meta_path\n",
    "print('running importer')\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9944,
     "status": "ok",
     "timestamp": 1596983406360,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "ManRVu7IsIjp",
    "outputId": "b48dd937-f2d5-4762-af1a-44fa03c44d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from utils import plot_decision_boundary, get_accuracy, get_prediction\n",
    "from utils import plot_2D_input_datapoints, generate_gifs, sigmoid, normalize\n",
    "import math\n",
    "import gif\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9918,
     "status": "ok",
     "timestamp": 1596983406361,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "xbV2U06Cs45b"
   },
   "outputs": [],
   "source": [
    "# Let's initialize our weights using uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = y.shape#1 #len(np.unique(y))\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36195,
     "status": "ok",
     "timestamp": 1596983432936,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "SAAbK03fLCR1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "# One hot encoding of our output label vector y\n",
    "def one_hot(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "# Loading dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "#Y = one_hot(Y)\n",
    "\n",
    "# Absorbing weight b of the hyperplane\n",
    "X = digits.data\n",
    "b_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 65), array([0, 1, 2, ..., 8, 9, 8]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36177,
     "status": "ok",
     "timestamp": 1596983432939,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "yzdjTbEYLvPK",
    "outputId": "76ed5c87-3298-433d-cf76-d68026c46342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC8pJREFUeJzt3d+LXPUZx/HPxzXijyQuRCtqxFQoARG6CRIqAWkTlVgletGLBCpEWtKLVgwNiPamyT8g6UURQtQEjBGNBoq01oAuIrTaJK41urGYEHEbdf1BSGKhQfP0Yk5KDNvu2WW/35nZ5/2CITO7Z+Z5djefOefMnDmPI0IAcrmg2w0AqI/gAwkRfCAhgg8kRPCBhAg+kFBPBN/2Ktvv2/7A9sOFaz1he9z2wZJ1zql3ne1XbY/aftf2g4XrXWz7TdtvN/U2l6zX1Byw/ZbtF0vXauodtf2O7RHb+wrXGrS92/ah5m94S8Fai5uf6ezlhO0NRYpFRFcvkgYkHZZ0g6SLJL0t6caC9W6VtFTSwUo/39WSljbX50n6R+Gfz5LmNtfnSHpD0g8K/4y/lvS0pBcr/U6PSrqiUq0dkn7eXL9I0mClugOSPpF0fYnH74U1/jJJH0TEkYg4LekZSfeUKhYRr0n6stTjT1Dv44g40Fw/KWlU0rUF60VEnGpuzmkuxY7Ssr1Q0l2StpWq0S2256uzonhckiLidEQcr1R+paTDEfFhiQfvheBfK+mjc26PqWAwusn2IklL1FkLl6wzYHtE0rikvRFRst4WSQ9JOlOwxvlC0su299teX7DODZI+k/RksyuzzfZlBeuda42kXaUevBeC7wm+NuuOI7Y9V9LzkjZExImStSLim4gYkrRQ0jLbN5WoY/tuSeMRsb/E4/8fyyNiqaQ7Jf3S9q2F6lyozm7hYxGxRNJXkoq+BiVJti+StFrSc6Vq9ELwxyRdd87thZKOdamXImzPUSf0OyPihVp1m83SYUmrCpVYLmm17aPq7KKtsP1UoVr/FRHHmn/HJe1RZ3exhDFJY+dsMe1W54mgtDslHYiIT0sV6IXg/03S92x/t3mmWyPpD13uacbYtjr7iKMR8WiFelfaHmyuXyLpNkmHStSKiEciYmFELFLn7/ZKRPy0RK2zbF9me97Z65LukFTkHZqI+ETSR7YXN19aKem9ErXOs1YFN/OlzqZMV0XE17Z/JenP6ryS+UREvFuqnu1dkn4o6QrbY5J+GxGPl6qnzlrxPknvNPvdkvSbiPhjoXpXS9phe0CdJ/ZnI6LK22yVXCVpT+f5VBdKejoiXipY7wFJO5uV0hFJ9xesJduXSrpd0i+K1mneOgCQSC9s6gOojOADCRF8ICGCDyRE8IGEeir4hQ+/7Fot6lGv1+r1VPAl1fzlVv1DUo96vVSv14IPoIIiB/DYntVHBQ0MDEz5PmfOnNEFF0zvefaaa66Z8n1OnTqluXPnTqveggULpnyfL774Ylr3k6STJ09O+T4nTpzQ/Pnzp1Xv8OHD07pfv4iIiT749i1dP2S3H82bN69qvY0bN1att27duqr1hoeHq9a79957q9brRWzqAwkRfCAhgg8kRPCBhAg+kBDBBxIi+EBCBB9IqFXwa464AlDepMFvTtr4e3VO+XujpLW2byzdGIBy2qzxq464AlBem+CnGXEFZNHmQzqtRlw1Jw6o/ZllANPQJvitRlxFxFZJW6XZ/7FcoN+12dSf1SOugIwmXePXHnEFoLxWJ+Jo5ryVmvUGoDKO3AMSIvhAQgQfSIjgAwkRfCAhgg8kRPCBhAg+kBCTdKZh+/btVevdc0/dT0Fv3ry5ar3ak3tq16v9/6UN1vhAQgQfSIjgAwkRfCAhgg8kRPCBhAg+kBDBBxIi+EBCBB9IqM0IrSdsj9s+WKMhAOW1WeNvl7SqcB8AKpo0+BHxmqQvK/QCoBL28YGEZuxjuczOA/rHjAWf2XlA/2BTH0iozdt5uyT9RdJi22O2f1a+LQAltRmaubZGIwDqYVMfSIjgAwkRfCAhgg8kRPCBhAg+kBDBBxIi+EBCs2J23qJFi6rWqz3LbseOHVXrbdq0qWq9wcHBqvWGhoaq1utFrPGBhAg+kBDBBxIi+EBCBB9IiOADCRF8ICGCDyRE8IGECD6QUJuTbV5n+1Xbo7bftf1gjcYAlNPmWP2vJW2MiAO250nab3tvRLxXuDcAhbSZnfdxRBxorp+UNCrp2tKNAShnSvv4thdJWiLpjRLNAKij9cdybc+V9LykDRFxYoLvMzsP6BOtgm97jjqh3xkRL0y0DLPzgP7R5lV9S3pc0mhEPFq+JQCltdnHXy7pPkkrbI80lx8X7gtAQW1m570uyRV6AVAJR+4BCRF8ICGCDyRE8IGECD6QEMEHEiL4QEIEH0hoVszOO378eLdbKGr79u3dbqGo2f7360Ws8YGECD6QEMEHEiL4QEIEH0iI4AMJEXwgIYIPJETwgYQIPpBQm7PsXmz7TdtvN7PzNtdoDEA5bY7V/7ekFRFxqjm//uu2/xQRfy3cG4BC2pxlNySdam7OaS4MzAD6WKt9fNsDtkckjUvaGxHMzgP6WKvgR8Q3ETEkaaGkZbZvOn8Z2+tt77O9b6abBDCzpvSqfkQclzQsadUE39saETdHxM0z1BuAQtq8qn+l7cHm+iWSbpN0qHRjAMpp86r+1ZJ22B5Q54ni2Yh4sWxbAEpq86r+3yUtqdALgEo4cg9IiOADCRF8ICGCDyRE8IGECD6QEMEHEiL4QEKzYnbe0NBQt1sA+gprfCAhgg8kRPCBhAg+kBDBBxIi+EBCBB9IiOADCRF8ICGCDyTUOvjNUI23bHOiTaDPTWWN/6Ck0VKNAKin7QithZLukrStbDsAami7xt8i6SFJZwr2AqCSNpN07pY0HhH7J1mO2XlAn2izxl8uabXto5KekbTC9lPnL8TsPKB/TBr8iHgkIhZGxCJJayS9EhE/Ld4ZgGJ4Hx9IaEqn3oqIYXXGZAPoY6zxgYQIPpAQwQcSIvhAQgQfSIjgAwkRfCAhgg8kNCtm542MjHS7haIuv/zyqvUGBwer1qs9+3DTpk1V6/Ui1vhAQgQfSIjgAwkRfCAhgg8kRPCBhAg+kBDBBxIi+EBCBB9IqNUhu82ptU9K+kbS15xCG+hvUzlW/0cR8XmxTgBUw6Y+kFDb4Iekl23vt72+ZEMAymu7qb88Io7Z/o6kvbYPRcRr5y7QPCHwpAD0gVZr/Ig41vw7LmmPpGUTLMPsPKBPtJmWe5nteWevS7pD0sHSjQEop82m/lWS9tg+u/zTEfFS0a4AFDVp8CPiiKTvV+gFQCW8nQckRPCBhAg+kBDBBxIi+EBCBB9IiOADCRF8ICFHxMw/qD3zD9pDhoeHu91CUUePHu12C0WtW7eu2y0UFRGebBnW+EBCBB9IiOADCRF8ICGCDyRE8IGECD6QEMEHEiL4QEIEH0ioVfBtD9rebfuQ7VHbt5RuDEA5bQdq/E7SSxHxE9sXSbq0YE8ACps0+LbnS7pV0jpJiojTkk6XbQtASW029W+Q9JmkJ22/ZXtbM1jjW2yvt73P9r4Z7xLAjGoT/AslLZX0WEQskfSVpIfPX4gRWkD/aBP8MUljEfFGc3u3Ok8EAPrUpMGPiE8kfWR7cfOllZLeK9oVgKLavqr/gKSdzSv6RyTdX64lAKW1Cn5EjEhi3x2YJThyD0iI4AMJEXwgIYIPJETwgYQIPpAQwQcSIvhAQszOm4bBwcGq9bZs2VK13tDQUNV6tWfZjYyMVK1XG7PzAEyI4AMJEXwgIYIPJETwgYQIPpAQwQcSIvhAQgQfSGjS4NtebHvknMsJ2xtqNAegjEnPuRcR70sakiTbA5L+KWlP4b4AFDTVTf2Vkg5HxIclmgFQx1SDv0bSrhKNAKindfCbc+qvlvTc//g+s/OAPtF2oIYk3SnpQER8OtE3I2KrpK3S7P9YLtDvprKpv1Zs5gOzQqvg275U0u2SXijbDoAa2o7Q+pekBYV7AVAJR+4BCRF8ICGCDyRE8IGECD6QEMEHEiL4QEIEH0iI4AMJlZqd95mk6Xxm/wpJn89wO71Qi3rUq1Xv+oi4crKFigR/umzvi4ibZ1st6lGv1+qxqQ8kRPCBhHot+FtnaS3qUa+n6vXUPj6AOnptjQ+gAoIPJETwgYQIPpAQwQcS+g8Vb4uzxFRLoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig()\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36148,
     "status": "ok",
     "timestamp": 1596983432942,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "3CIYTv4x65As",
    "outputId": "d9f59ee0-8392-4ba9-8cb8-11afc1669fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1308, 65)\n",
      "Validation dataset:  (188, 65)\n",
      "Test dataset:  (301, 65)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test set.\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.167)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36132,
     "status": "ok",
     "timestamp": 1596983432945,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "d3NzkO4s68RX"
   },
   "outputs": [],
   "source": [
    "# Normalizing X_train and absorbing weight b of the hyperplane\n",
    "X_normalized_train = normalize(X_train[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_train), 1))\n",
    "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized_test = normalize(X_test[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_test), 1))\n",
    "X_normalized_test = np.hstack((X_normalized_test, b_ones))\n",
    "\n",
    "X_normalized_val = normalize(X_val[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_val), 1))\n",
    "X_normalized_val = np.hstack((X_normalized_val, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36096,
     "status": "ok",
     "timestamp": 1596983432947,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "pYrK4fK3iyyk",
    "outputId": "a73d5605-db30-4099-f72e-9cca8e2fe3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1308, 65), (1308, 65), (301, 65), (188, 65))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_train.shape, X_train.shape, X_normalized_test.shape, X_normalized_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs All using Tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_vs_all(X_train, Y_train, base_weights, num_epochs=1000, learning_rate=0.1):\n",
    "\n",
    "    \"\"\"Method to train a multiclass classifier using one vs all technique.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "\n",
    "    X_train: ndarray (num_examples(rows) vs num_columns(columns))\n",
    "    Input data on which the logistic regression model will be trained \n",
    "    to acquire optimal weights\n",
    "\n",
    "    Y_train: ndarray (num_examples(rows) vs class_labels(columns))\n",
    "    Class labels of training set\n",
    "\n",
    "    base_weights: ndarray (num_features vs n_outputs)\n",
    "    Weights used to train the network and predict on test set\n",
    "\n",
    "    num_epochs: int\n",
    "    Number of epochs you want to train the model\n",
    "\n",
    "    learning_rate: int\n",
    "    rate with which weights will be update every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = np.shape(X_train)\n",
    "    n_outputs = len(np.unique(Y_train))\n",
    "\n",
    "    history_weights = []\n",
    "    weights_k_classes = []\n",
    "    epoch = 1\n",
    "\n",
    "    classes = np.unique(Y_train)\n",
    "\n",
    "    # Training using Batch GD\n",
    "    for k in classes:\n",
    "        # one-vs-all binary classifier\n",
    "        binary_y_train = np.where(Y_train == k, 1, 0)\n",
    "        #print(\"binary_y_train \", binary_y_train.shape, Y_train.shape)\n",
    "\n",
    "        weights = base_weights\n",
    "        print('class...', k )\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            #print('epoch...', epoch )\n",
    "        \n",
    "    \n",
    "            # Computing weighted inputs and predicting output\n",
    "            w_transpose_x = np.dot(X_train, weights)\n",
    "             \n",
    "            y_pred = sigmoid(w_transpose_x)\n",
    "            #print(y_pred)\n",
    "            #print(\"weights, X_train , w_transpose_x,  y_pred shape\" , weights.shape,X_train.shape, w_transpose_x.shape ,  y_pred.shape)\n",
    "            \n",
    "            # Calculating gradient and updating weights\n",
    "            #print(\"binary_y_train , y_pred\" , binary_y_train.shape , y_pred.shape)\n",
    "            gradient = 1 * np.dot(X_train.T, (binary_y_train - y_pred))\n",
    "            #print(gradient)\n",
    "            weights += (learning_rate/n_samples) * gradient\n",
    "            weights = np.round(weights, decimals=7)\n",
    "            #print(\"gradient , weights \",gradient.shape, weights.shape)\n",
    "            #weights_k_classes.append(weights)\n",
    "            epoch += 1\n",
    " \n",
    "        #history_weights.append(weights_k_classes)\n",
    "        history_weights.append(weights)\n",
    "        #weights_k_classes = []\n",
    "\n",
    "    #print(\"Training complete\")\n",
    "    print(\"Training complete, history_weights\", np.array(history_weights).shape)\n",
    "    return history_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65, 1),)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing weights from uniform distribution\n",
    "weights = weight_init_uniform_dist(X_train, Y_train.reshape((len(Y_train)), 1))\n",
    "weights.shape, #np.shape(Y_train.reshape((len(Y_train)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class... 0\n",
      "class... 1\n",
      "class... 2\n",
      "class... 3\n",
      "class... 4\n",
      "class... 5\n",
      "class... 6\n",
      "class... 7\n",
      "class... 8\n",
      "class... 9\n",
      "Training complete, history_weights (10, 65, 1)\n"
     ]
    }
   ],
   "source": [
    "trained_weights = train_one_vs_all(X_train, Y_train.reshape((len(Y_train)), 1), weights, num_epochs=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of training data and validation data\n",
    "def predict_one_vs_all(trained_weights, X_input, Y_input):\n",
    "  \n",
    "    num_classes = len(np.unique(Y_input))\n",
    "    scores = np.zeros((num_classes, (X_input.shape)[0]))\n",
    "\n",
    "    for k in range(num_classes):\n",
    "    \n",
    "        binary_y_input = np.where(Y_input == k, 1, 0)\n",
    "\n",
    "        w_transpose_x = np.dot(X_input, trained_weights[k])\n",
    "        y_pred = sigmoid(w_transpose_x)\n",
    "        y_pred = y_pred.reshape((-1,))\n",
    "        scores[k, :] = y_pred\n",
    "\n",
    "    pred_X = np.argmax(scores, axis=0)\n",
    "    return pred_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10247127515148839"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_test_one_vs_all==Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy: 0.100161\n",
      "Validation dataset accuracy: 0.101913\n",
      "Test datast accuracy: 0.102990\n"
     ]
    }
   ],
   "source": [
    "pred_train_one_vs_all = predict_one_vs_all(trained_weights, X_normalized_train, Y_train)\n",
    "pred_val_one_vs_all = predict_one_vs_all(trained_weights, X_normalized_val, Y_val)\n",
    "pred_test_one_vs_all = predict_one_vs_all(trained_weights, X_normalized_test, Y_test)\n",
    "\n",
    "pred_train_one_vs_all = pred_train_one_vs_all.reshape((-1, 1))\n",
    "pred_val_one_vs_all = pred_val_one_vs_all.reshape((-1, 1))\n",
    "pred_test_one_vs_all = pred_test_one_vs_all.reshape((-1, 1))\n",
    "\n",
    "print('Training dataset accuracy: %f' % (np.mean(Y_train == pred_train_one_vs_all)))\n",
    "print('Validation dataset accuracy: %f' % (np.mean(Y_val == pred_val_one_vs_all)))\n",
    "print('Test datast accuracy: %f' % (np.mean(Y_test == pred_test_one_vs_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 using optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 65)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# data = sio.loadmat(\"digits.mat\")\n",
    "# x = data['X'] # the feature matrix is labeled with 'X' inside the file\n",
    "# y = np.squeeze(data['y']) # the target variable vector is labeled with 'y' inside the file\n",
    "#np.place(y, y == 10, 0) # replace the label 10 with 0\n",
    "numExamples = X.shape[0] # 1797 examples\n",
    "numFeatures = X.shape[1] # 65 features\n",
    "numLabels = 10 # digits from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    predictions[predictions == 1] = 0.999 # log(1)=0 causes error in division\n",
    "    error = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions);\n",
    "    return sum(error) / len(y);\n",
    "\n",
    "def cost_gradient(theta, X, y):\n",
    "    predictions = sigmoid(X @ theta);\n",
    "    return X.transpose() @ (predictions - y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.ones(shape=(X.shape[0], X.shape[1] + 1))\n",
    "X_data[:, 1:] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = np.zeros(shape=(numLabels, numFeatures + 1))\n",
    "for c in range(0, numLabels):\n",
    "    label = (Y == c).astype(int)\n",
    "    initial_theta = np.zeros(X_data.shape[1])\n",
    "    classifiers[c, :] = opt.fmin_cg(cost, initial_theta, cost_gradient, (X_data, label), disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "classProbabilities = sigmoid(X_data @ classifiers.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classProbabilities.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.88870339454647%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy:\", str(100 * np.mean(predictions == Y)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs One approach Using Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the dataset\n",
    "def one_vs_one_train(X_train, Y_train, weights, num_epochs=1000, learning_rate=0.1):\n",
    "\n",
    "    \"\"\" Method to train a logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "\n",
    "    X_train: ndarray (num_examples(rows) vs num_columns(columns))\n",
    "    Input data on which the logistic regression model will be trained \n",
    "    to acquire optimal weights\n",
    "\n",
    "    Y_train: ndarray (num_examples(rows) vs class_labels(columns))\n",
    "    Class labels of training set\n",
    "\n",
    "    weights: ndarray (num_features vs n_output)\n",
    "    Weights used to train the network and predict on test set\n",
    "\n",
    "    num_epochs: int\n",
    "    Number of epochs you want to train the model\n",
    "\n",
    "    learning_rate: int\n",
    "    rate with which weights will be update every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = np.shape(X_train)\n",
    "    _, n_outputs = np.shape(Y_train)\n",
    "\n",
    "    history_weights = []\n",
    "    epoch = 1\n",
    "\n",
    "    # Training using Batch GD\n",
    "    while epoch <= num_epochs:\n",
    "\n",
    "        # Appending previous history weights/first initialized weights\n",
    "        history_weights.append(weights)\n",
    "\n",
    "        # Computing weighted inputs and predicting output\n",
    "        w_transpose_x = np.dot(X_train, weights)\n",
    "        y_pred = sigmoid(w_transpose_x)\n",
    "        #print('w_transpose_x , y_pred', w_transpose_x.shape , y_pred.shape)\n",
    "        # Calculating gradient and updating weights\n",
    "        gradient = 1 * np.dot(X_train.T, (Y_train - y_pred))\n",
    "        weights += (learning_rate/n_samples) * gradient\n",
    "        weights = np.round(weights, decimals=7)\n",
    "        #print('gradient , weights', gradient.shape , weights.shape)\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Training complete\")\n",
    "\n",
    "    return history_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308,)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Initializing weights from uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = np.shape(y)\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights\n",
    "\n",
    "weights = weight_init_uniform_dist(X_train, Y_train.reshape((len(Y_train)), 1))\n",
    "\n",
    "trained_weights = one_vs_one_train(X_train, Y_train.reshape((len(Y_train)), 1), weights, num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n",
      "Training accuracy: 10.168\n",
      "Validation accuracy: 11.702\n",
      "Test accuracy: 8.970\n"
     ]
    }
   ],
   "source": [
    "best_weights = trained_weights[-1]\n",
    "\n",
    "print(\"Evaluation results\")\n",
    "train_acc, _ = get_prediction(X_train, Y_train.reshape((len(Y_train)), 1), best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "val_acc, _ = get_prediction(X_val, Y_val.reshape((len(Y_val)), 1), best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "test_acc, _ = get_prediction(X_test, Y_test.reshape((len(Y_test)), 1), best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "\n",
    "print(\"Training accuracy: {:.3f}\" .format(train_acc))\n",
    "print(\"Validation accuracy: {:.3f}\" .format(val_acc))\n",
    "print(\"Test accuracy: {:.3f}\" .format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs Rest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs rest accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "OVR = OneVsRestClassifier(LogisticRegression()).fit(X_train,Y_train)\n",
    "print(\"One vs rest accuracy: %.3f\" % OVR.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs One "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs one accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "OVO = OneVsOneClassifier(LogisticRegression()).fit(X_train,Y_train)\n",
    "print(\"One vs one accuracy: %.3f\" % OVO.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref https://www.dummies.com/programming/big-data/data-science/using-logistic-regression-in-python-for-data-science/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LogisticRegression_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
